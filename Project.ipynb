{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdzeFNwL1V94"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install libsdl2-gfx-dev libsdl2-ttf-dev\n",
        "\n",
        "# Make sure that the Branch in git clone and in wget call matches !!\n",
        "!git clone -b v2.9 https://github.com/google-research/football.git\n",
        "!mkdir -p football/third_party/gfootball_engine/lib\n",
        "\n",
        "!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n",
        "!cd football && GFOOTBALL_USE_PREBUILT_SO=1 python3 -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaAvHjvPyXCw"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSkjgg0hr0Vq"
      },
      "source": [
        "import gfootball.env as football_env\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjqSd1vTe0U"
      },
      "source": [
        "def get_actor(dims_in, dims_out):\n",
        "  \n",
        "  input = Input(shape = dims_in, name = 'Input')\n",
        "  old_probs = Input(shape = (1, dims_out, ), name = 'Old_probabilities')\n",
        "  advantages = Input(shape = (1, 1, ), name = 'Advantages')\n",
        "  q_values = Input(shape = (1, 1, ), name = 'Q-values')\n",
        "  rewards = Input(shape = (1, 1, ), name = 'Rewards')\n",
        "\n",
        "  x = Dense(256, activation = 'tanh')(input)\n",
        "  x = Dense(128, activation = 'tanh')(x)\n",
        "  output = Dense(n_actions, activation = 'softmax', name = 'Predictions')(x)\n",
        "\n",
        "  model = Model(inputs = [input, old_probs, advantages, q_values, rewards], outputs = [output])\n",
        "  model.compile(optimizer=Adam(lr = learning_rate), loss = get_ppo_loss(old_probs, advantages, q_values, rewards))\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def get_critic(dims_in):\n",
        "  \n",
        "  input = Input(shape = dims_in, name = 'Input')\n",
        "\n",
        "  x = Dense(256, activation = 'tanh')(input)\n",
        "  x = Dense(128, activation = 'tanh')(x)\n",
        "  output = Dense(1, name = 'Predictions')(x)\n",
        "\n",
        "  model = Model(inputs = input, outputs = output)\n",
        "  model.compile(optimizer=Adam(lr = learning_rate), loss = 'mse')\n",
        "\n",
        "  return model\n",
        "\n",
        "def get_ppo_loss(old_probs, advantages, q_values, rewards):\n",
        "  \n",
        "  def get_loss(y_true, y_pred):\n",
        "    \n",
        "    new_probs = y_pred\n",
        "\n",
        "    r = K.exp(K.log(new_probs + 1e-10) - K.log(old_probs + 1e-10))\n",
        "\n",
        "    actor_loss = -K.mean(K.minimum(r * advantages, \n",
        "        K.clip(r, min_value=1 - loss_clipping, max_value=1 + loss_clipping) * advantages))\n",
        "    \n",
        "    critic_loss = K.mean(K.square(rewards - q_values))\n",
        "\n",
        "    total_loss = critic_discount * critic_loss + actor_loss - entropy_loss * K.mean(\n",
        "        -(new_probs * K.log(new_probs + 1e-10)))\n",
        "    \n",
        "    return total_loss\n",
        "  \n",
        "  return get_loss\n",
        "\n",
        "def get_normalize(x):\n",
        "  \n",
        "  mean = np.mean(x)\n",
        "  \n",
        "  std = np.std(x)\n",
        "  \n",
        "  return (x - mean) / (std + 1e-10)\n",
        "\n",
        "def get_advantages(q_values, statuses, rewards):\n",
        "    \n",
        "  returns = []\n",
        "  \n",
        "  gae = 0\n",
        "\n",
        "  for i in reversed(range(len(rewards))):\n",
        "    \n",
        "    delta = rewards[i] + gamma * q_values[i + 1] * statuses[i] - q_values[i]\n",
        "    \n",
        "    gae = delta + gamma * lambda_ * statuses[i] * gae\n",
        "    \n",
        "    returns.insert(0, gae + q_values[i])  \n",
        "\n",
        "  adv = np.array(returns) - q_values[:-1]\n",
        "  \n",
        "  return returns, (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
        "\n",
        "def get_empty_arrays():\n",
        "  # create an empty array to store states\n",
        "  states = []\n",
        "\n",
        "  # create an empty array to store actions \n",
        "  actions = []\n",
        "\n",
        "  # create an empty array to store values from critic model\n",
        "  q_values = []\n",
        "\n",
        "  # create an empty array to store if game is over or completed\n",
        "  statuses = []\n",
        "\n",
        "  # create an empty array to store rewards\n",
        "  rewards = []\n",
        "  \n",
        "  # create an empty array to store one-hot-encoded actions\n",
        "  actions_ohe = []\n",
        "  \n",
        "  # create an empty array to store probabilities of actions\n",
        "  actions_probs = []\n",
        "\n",
        "  return states, actions, q_values, statuses, rewards, actions_ohe, actions_probs\n",
        "\n",
        "def get_action(state_in):\n",
        "    \n",
        "  actions_dist = model_actor.predict([state_in, dummy_1, dummy_2, dummy_2, dummy_2], steps = 1)\n",
        "  \n",
        "  action = np.random.choice(n_actions, p = actions_dist[0, :])\n",
        "  \n",
        "  action_ohe = np.zeros(n_actions)\n",
        "  \n",
        "  action_ohe[action] = 1\n",
        "\n",
        "  return action, action_ohe, actions_dist\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ELssXkLsLSA"
      },
      "source": [
        "# create an environment academy_empty_goal_close\n",
        "env = football_env.create_environment(env_name=\"academy_empty_goal\",\n",
        "                                      stacked=False, logdir='/tmp/football', \n",
        "                                      write_goal_dumps=False, \n",
        "                                      write_full_episode_dumps=False, \n",
        "                                      render=False, representation = 'simple115v2',\n",
        "                                      rewards = 'scoring')\n",
        "\n",
        "\n",
        "# get the dimensions of the observation space in the environment\n",
        "obs_space = env.observation_space.shape\n",
        "#print(obs_space)\n",
        "\n",
        "# get the number of actions in the environment\n",
        "n_actions = env.action_space.n\n",
        "#print(n_actions)\n",
        "\n",
        "#hyperparameters\n",
        "loss_clipping = 0.2\n",
        "entropy_loss = 0.005\n",
        "gamma = 0.99\n",
        "lambda_ = 0.95\n",
        "critic_discount = 0.5\n",
        "learning_rate = 1e-4\n",
        "\n",
        "\n",
        "#dummy values\n",
        "dummy_1 = np.zeros((1, 1, n_actions))\n",
        "dummy_2 = np.zeros((1, 1, 1))\n",
        "\n",
        "#get models\n",
        "model_actor = get_actor(obs_space, n_actions)\n",
        "model_critic = get_critic(obs_space)\n",
        "\n",
        "episode = 0\n",
        "episodes = 200\n",
        "ppo_steps = 128\n",
        "epochs = 5\n",
        "best_rew = 0\n",
        "all_rewards = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY7jpl7ehAdG"
      },
      "source": [
        "state = env.reset()\n",
        "\n",
        "while episode <= episodes:\n",
        "  print(episode)\n",
        "  \n",
        "  states, actions, q_values, statuses, rewards, actions_ohe, actions_probs = get_empty_arrays()\n",
        "\n",
        "  state_in = None\n",
        "  \n",
        "  for i in range(ppo_steps):\n",
        "    \n",
        "    state_in = K.expand_dims(state, 0)\n",
        "    \n",
        "    action, action_ohe, actions_dist = get_action(state_in)\n",
        "    \n",
        "    q_value = model_critic.predict(state_in, steps = 1)\n",
        "    \n",
        "    obs, rew, done, info = env.step(action)\n",
        "    \n",
        "    print(\"Step\", i, \"; Action\", action, \"; Reward\", rew, \"; Q-value\", q_value[0])\n",
        "\n",
        "    status = not done\n",
        "\n",
        "    states.append(state)\n",
        "    \n",
        "    actions.append(action)\n",
        "    \n",
        "    actions_ohe.append(action_ohe)\n",
        "    \n",
        "    q_values.append(q_value)\n",
        "    \n",
        "    statuses.append(status)\n",
        "    \n",
        "    rewards.append(rew)\n",
        "    \n",
        "    actions_probs.append(actions_dist)\n",
        "    \n",
        "    state = obs\n",
        "\n",
        "    if done:\n",
        "      env.reset()\n",
        "  \n",
        "  all_rewards.append(sum(rewards))\n",
        "\n",
        "  q_value = model_critic.predict(state_in, steps = 1)\n",
        "  q_values.append(q_value)\n",
        "\n",
        "  returns, advantages = get_advantages(q_values, statuses, rewards)\n",
        "\n",
        "  actor_loss = model_actor.fit(\n",
        "          [states, \n",
        "          actions_probs, \n",
        "          advantages,\n",
        "          q_values[:-1],\n",
        "          np.reshape(rewards, newshape = (-1, 1, 1))\n",
        "          ],\n",
        "          [(np.reshape(actions_ohe, newshape = (-1, n_actions)))], \n",
        "          verbose=1, \n",
        "          shuffle=False, \n",
        "          epochs=epochs)\n",
        "  \n",
        "  critic_loss = model_critic.fit(\n",
        "          [states], \n",
        "          [np.reshape(returns, newshape=(-1, 1))],\n",
        "          verbose=1,\n",
        "          shuffle=False, \n",
        "          epochs=epochs)\n",
        "\n",
        "  if episode % 5 == 0:\n",
        "    model_actor.save('model_actor2.hdf5')\n",
        "    model_critic.save('model_critic2.hdf5')\n",
        "    print(all_rewards)\n",
        "\n",
        "  if episode == episodes:\n",
        "    print('Done')\n",
        "\n",
        "  episode += 1\n",
        "  env.reset()      \n",
        "\n",
        "# close environment\n",
        "env.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNSert6Wy7mX"
      },
      "source": [
        "def get_test_reward_dist(model, lim):\n",
        "    \n",
        "    state = env.reset()\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    total_reward = 0\n",
        "    \n",
        "    limit = 0\n",
        "    \n",
        "    while not done:\n",
        "        \n",
        "        state_input = K.expand_dims(state, 0)\n",
        "        \n",
        "        action_probs = model.predict([state_input, dummy_1, dummy_2, dummy_2, dummy_2], steps=1)\n",
        "        \n",
        "        action = np.random.choice(n_actions, p = action_probs[0, :])\n",
        "        \n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        state = next_state\n",
        "        \n",
        "        total_reward += reward\n",
        "        \n",
        "        limit += 1\n",
        "        \n",
        "        if limit > lim:         \n",
        "            break\n",
        "    \n",
        "    return total_reward\n",
        "\n",
        "\n",
        "env = football_env.create_environment(env_name=\"academy_empty_goal\",\n",
        "                                      stacked=False, logdir='/tmp/football', \n",
        "                                      write_goal_dumps=False, \n",
        "                                      write_full_episode_dumps=False, \n",
        "                                      render=False, representation = 'simple115v2',\n",
        "                                      rewards = 'scoring, checkpoint')\n",
        "\n",
        "test_model_actor = load_model('/content/drive/MyDrive/Colab Notebooks/RL project/Actor-Critic/model_actor_11.06.hdf5', compile = False)\n",
        "\n",
        "avg_rew = np.mean([get_test_reward_dist(test_model_actor, 128) for _ in range(20)])\n",
        "\n",
        "print('Average test reward:', avg_rew)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}